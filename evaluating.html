<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Training and Evaluating Agents &mdash; BEHAVIOR 0.0.1 documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Training Agents" href="benchmarking.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> BEHAVIOR
          </a>
              <div class="version">
                0.0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Introduction</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">The BEHAVIOR Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training and Evaluating Agents</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#evaluating-an-agent-on-behavior-after-a-manual-installation">Evaluating an agent on BEHAVIOR after a manual installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#evaluating-on-a-single-activity-instance">Evaluating on a single activity instance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluating-an-agent-on-behavior-after-a-docker-installation">Evaluating an agent on BEHAVIOR after a Docker installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#submitting-to-the-behavior-public-leaderboard-on-evalai">Submitting to the BEHAVIOR public leaderboard on EvalAI</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Training Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="baselines.html">Baselines</a></li>
</ul>
<p class="caption"><span class="caption-text">Components of BEHAVIOR</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="agents.html">Embodiments: actuation, sensing, grasping</a></li>
<li class="toctree-l1"><a class="reference internal" href="setups.html">Benchmark Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="activities.html">The BEHAVIOR Dataset of Activity Definitions and BDDL</a></li>
<li class="toctree-l1"><a class="reference internal" href="vr_demos.html">The BEHAVIOR Dataset of Human Demonstrations in Virtual Reality</a></li>
<li class="toctree-l1"><a class="reference internal" href="objects.html">The BEHAVIOR Dataset of Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BEHAVIOR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Training and Evaluating Agents</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/evaluating.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="training-and-evaluating-agents">
<h1>Training and Evaluating Agents<a class="headerlink" href="#training-and-evaluating-agents" title="Permalink to this headline"></a></h1>
<p>Once you have installed BEHAVIOR and its dependencies, you are ready to evaluate your agents on the benchmark. To be evaluated, agents have to be instantiated as objects of the class <code class="docutils literal notranslate"><span class="pre">Agent</span></code> or derived. We have created an initial derived class <code class="docutils literal notranslate"><span class="pre">CustomAgent</span></code> in <code class="docutils literal notranslate"><span class="pre">behavior/benchmark/agents/users_agent.py</span></code> to be fulfilled for your agent. The minimal functionality necessary for your agent is a <code class="docutils literal notranslate"><span class="pre">reset</span></code> function and the <code class="docutils literal notranslate"><span class="pre">act</span></code> action that provides actions at each timestep, given an observation from the environment.</p>
<p>Once your agent is implemented as derived from the <code class="docutils literal notranslate"><span class="pre">Agent</span></code> class, you can evaluate it in BEHAVIOR activities. The way to do it will depend on your type of installation: manual or Docker installation. In the following, we include instructions for evaluation for both installation types.</p>
<div class="section" id="evaluating-an-agent-on-behavior-after-a-manual-installation">
<h2>Evaluating an agent on BEHAVIOR after a manual installation<a class="headerlink" href="#evaluating-an-agent-on-behavior-after-a-manual-installation" title="Permalink to this headline"></a></h2>
<p>To evaluate locally, use the functionalities in <code class="docutils literal notranslate"><span class="pre">behavior/benchmark/behavior_benchmark.py</span></code>. As an example, the following code benchmarks a random agent on a single activity indicated in the specified environment config file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">CONFIG_FILE</span><span class="o">=</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">your</span><span class="o">/</span><span class="n">config</span><span class="o">/</span><span class="k">for</span><span class="o">/</span><span class="n">example</span><span class="o">/</span><span class="n">behavior</span><span class="o">/</span><span class="n">configs</span><span class="o">/</span><span class="n">behavior_onboard_sensing</span><span class="o">.</span><span class="n">yaml</span>
<span class="n">export</span> <span class="n">OUTPUT_DIR</span><span class="o">=</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">your</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="nb">dir</span><span class="o">/</span><span class="k">for</span><span class="o">/</span><span class="n">example</span><span class="o">/</span><span class="n">tmp</span>
<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">behavior</span><span class="o">.</span><span class="n">benchmark</span><span class="o">.</span><span class="n">behavior_benchmark</span>
</pre></div>
</div>
<p>Once you have implemented your agent in the class <code class="docutils literal notranslate"><span class="pre">CustomAgent</span></code>, you can evaluate it instead of the random agent by changing the last command by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">behavior</span><span class="o">.</span><span class="n">benchmark</span><span class="o">.</span><span class="n">behavior_benchmark</span> <span class="o">--</span><span class="n">agent</span><span class="o">-</span><span class="k">class</span> <span class="nc">Custom</span>
</pre></div>
</div>
<p>The code in <code class="docutils literal notranslate"><span class="pre">behavior_benchmark.py</span></code> can be used to evaluate an agent following the official <a class="reference internal" href="setups.html"><span class="doc std std-doc">setup</span></a> benchmark rules: the agent is evaluated in the indicated activity/activities for nine instances of increasing complexity: three instances of the activity that were available for training, three instances where everything is the same as in training but the small objects change the objects initial locations, and three instances where the furniture in the scenes is also different. The code runs the benchmark metrics and saves the values on files in the <code class="docutils literal notranslate"><span class="pre">OUTPUT_DIR</span></code>.</p>
<p>The example above evaluates the random agent in a single activity specified in the environment’s config file. However, you can select the activity you want to benchmark the agent on with the option <code class="docutils literal notranslate"><span class="pre">--split</span></code> and the name of the activity (check all activities <a class="reference external" href="https://behavior.stanford.edu/activity_list.html">here</a> and video examples <a class="reference external" href="https://behavior.stanford.edu/behavior-gallery/activity.html">here</a>), or benchmark on the entire set of 100 activities by specifying <code class="docutils literal notranslate"><span class="pre">--split</span> <span class="pre">dev</span></code> or <code class="docutils literal notranslate"><span class="pre">--split</span> <span class="pre">test</span></code>, to use developing or test activity instances.</p>
<p>For example, to benchmark a provided PPO agent (reinforcement learning) loading a specific policy checkpoint only for the activity <code class="docutils literal notranslate"><span class="pre">cleaning_toilet</span></code>, you can execute:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">CONFIG_FILE</span><span class="o">=</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">your</span><span class="o">/</span><span class="n">config</span><span class="o">/</span><span class="k">for</span><span class="o">/</span><span class="n">example</span><span class="o">/</span><span class="n">behavior</span><span class="o">/</span><span class="n">configs</span><span class="o">/</span><span class="n">behavior_onboard_sensing</span><span class="o">.</span><span class="n">yaml</span>
<span class="n">export</span> <span class="n">OUTPUT_DIR</span><span class="o">=</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">your</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="nb">dir</span><span class="o">/</span><span class="k">for</span><span class="o">/</span><span class="n">example</span><span class="o">/</span><span class="n">tmp</span>
<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">behavior</span><span class="o">.</span><span class="n">benchmark</span><span class="o">.</span><span class="n">behavior_benchmark</span> <span class="o">--</span><span class="n">agent</span><span class="o">-</span><span class="k">class</span> <span class="nc">PPO</span> <span class="o">--</span><span class="n">ckpt</span><span class="o">-</span><span class="n">path</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">my_checkpoint</span> <span class="o">--</span><span class="n">split</span> <span class="n">cleaning_toilet</span>
</pre></div>
</div>
<p>We provide pretrained checkpoints in <code class="docutils literal notranslate"><span class="pre">behavior/benchmark/agents/checkpoints</span></code></p>
<div class="section" id="evaluating-on-a-single-activity-instance">
<h3>Evaluating on a single activity instance<a class="headerlink" href="#evaluating-on-a-single-activity-instance" title="Permalink to this headline"></a></h3>
<p>Instead of evaluating agents following the benchmark rules (nine instances per activity), you can also evaluate in one or a custom set of activity instances by calling directly the method <code class="docutils literal notranslate"><span class="pre">BehaviorBenchmark.evaluate_agent_on_one_activity</span></code> and providing a list of instances.</p>
</div>
</div>
<div class="section" id="evaluating-an-agent-on-behavior-after-a-docker-installation">
<h2>Evaluating an agent on BEHAVIOR after a Docker installation<a class="headerlink" href="#evaluating-an-agent-on-behavior-after-a-docker-installation" title="Permalink to this headline"></a></h2>
<p>We provide several scripts to evaluate agents on <code class="docutils literal notranslate"><span class="pre">minival</span></code> and <code class="docutils literal notranslate"><span class="pre">dev</span></code> splits. The <code class="docutils literal notranslate"><span class="pre">minival</span></code> split serves to evaluate on a single activity. The following code evaluates a random agent on the <code class="docutils literal notranslate"><span class="pre">minival</span></code> split using a local docker image:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">benchmark</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">test_minival_docker</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">docker</span><span class="o">-</span><span class="n">name</span> <span class="n">my_submission</span> <span class="o">--</span><span class="n">dataset</span><span class="o">-</span><span class="n">path</span> <span class="n">my</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">dataset</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">my_submission</span></code> is the name of the docker image, and <code class="docutils literal notranslate"><span class="pre">my/path/to/dataset</span></code> corresponds to the path to the iGibson and BEHAVIOR Datasets, and the <code class="docutils literal notranslate"><span class="pre">igibson.key</span></code> obtained following the <a class="reference internal" href="installation.html"><span class="doc std std-doc">installation instructions</span></a>.</p>
<p>You can also evaluate locally for the <code class="docutils literal notranslate"><span class="pre">dev</span></code> split (all activities) by executing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">benchmark</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">test_dev_docker</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">docker</span><span class="o">-</span><span class="n">name</span> <span class="n">my_submission</span> <span class="o">--</span><span class="n">dataset</span><span class="o">-</span><span class="n">path</span> <span class="n">my</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">dataset</span>
</pre></div>
</div>
<p>Both scripts call <code class="docutils literal notranslate"><span class="pre">behavior/benchmark/scripts/evaluate_agent.sh</span></code>. You can modify this script, or the docker scripts to evaluate your agent.</p>
<div class="section" id="submitting-to-the-behavior-public-leaderboard-on-evalai">
<h3>Submitting to the BEHAVIOR public leaderboard on EvalAI<a class="headerlink" href="#submitting-to-the-behavior-public-leaderboard-on-evalai" title="Permalink to this headline"></a></h3>
<p>If you use the Docker installation, you can submit your solution to be evaluated and included in the public leaderboard. For that, you first need to register for our benchmark on EvalAI <a class="reference external" href="https://eval.ai/web/challenges/challenge-page/1190/overview">here</a>. You should follow the instructions in the <code class="docutils literal notranslate"><span class="pre">submit</span></code> tab on EvalAI that we summarize here:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Installing EvalAI Command Line Interface</span>
<span class="n">pip</span> <span class="n">install</span> <span class="s2">&quot;evalai&gt;=1.2.3&quot;</span>

<span class="c1"># Set EvalAI account token</span>
<span class="n">evalai</span> <span class="n">set_token</span> <span class="o">&lt;</span><span class="n">your</span> <span class="n">EvalAI</span> <span class="n">participant</span> <span class="n">token</span><span class="o">&gt;</span>

<span class="c1"># Push docker image to EvalAI docker registry</span>
<span class="n">evalai</span> <span class="n">push</span> <span class="n">my_submission</span><span class="p">:</span><span class="n">latest</span> <span class="o">--</span><span class="n">phase</span> <span class="o">&lt;</span><span class="n">track</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>There are two valid benchmark tracks depending if your agent uses only onboard sensing or assumes full observability: <code class="docutils literal notranslate"><span class="pre">behavior-test-onboard-sensing-1190</span></code>, <code class="docutils literal notranslate"><span class="pre">behavior-test-full-observability-1190</span></code>.
Once we receive your submission, we evaluate and return the results.
Due to the time and resource consuming evaluation process, each participant is restricted to submit once per week, maximum 4 times per month.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="benchmarking.html" class="btn btn-neutral float-right" title="Training Agents" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Stanford University 2018-2021.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>