<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The BEHAVIOR Dataset of Human Demonstrations in Virtual Reality &mdash; BEHAVIOR 0.0.1 documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The BEHAVIOR Dataset of Objects" href="objects.html" />
    <link rel="prev" title="The BEHAVIOR Dataset of Activity Definitions and BDDL" href="activities.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> BEHAVIOR
          </a>
              <div class="version">
                0.0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro.html">The BEHAVIOR Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Training and Evaluating Agents with BEHAVIOR</a></li>
<li class="toctree-l1"><a class="reference internal" href="baselines.html">Baselines</a></li>
</ul>
<p class="caption"><span class="caption-text">Components of BEHAVIOR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="agents.html">Embodiments: actuation, sensing, grasping</a></li>
<li class="toctree-l1"><a class="reference internal" href="setups.html">Benchmark Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="activities.html">The BEHAVIOR Dataset of Activity Definitions and BDDL</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">The BEHAVIOR Dataset of Human Demonstrations in Virtual Reality</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#behavior-hdf5-processed-dataset">BEHAVIOR HDF5 Processed Dataset</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hdf5-content">HDF5 Content</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#behavior-hdf5-raw-dataset">BEHAVIOR HDF5 Raw Dataset</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#accessing-the-behavior-hdf5-demo-files">Accessing the BEHAVIOR hdf5 demo files</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metadata">Metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">HDF5 Content</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#add-your-own-demos">Add your own demos!</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="objects.html">The BEHAVIOR Dataset of Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BEHAVIOR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>The BEHAVIOR Dataset of Human Demonstrations in Virtual Reality</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/vr_demos.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="the-behavior-dataset-of-human-demonstrations-in-virtual-reality">
<h1>The BEHAVIOR Dataset of Human Demonstrations in Virtual Reality<a class="headerlink" href="#the-behavior-dataset-of-human-demonstrations-in-virtual-reality" title="Permalink to this headline"></a></h1>
<p>The BEHAVIOR dataset includes 500 demonstrations of human participants performing BEHAVIOR-100 activities in iGibson 2.0 using a virtual reality interface. The data was collected by 5 participants using an HTC Vive Pro Eye headset with 2 HTC Vive Wand controllers and 1 torso tracker strapped to the participant’s waist. The participants were cued by a VR overlay describing activity goal conditions. Activity relevant objects corresponding to a goal condition were highlighted on the user view on-demand, guiding human execution. The text of the corresponding part of the condition switched from red to green at the completion of a given part.</p>
<p>There are two available HDF5 datasets. The first is processed for imitation learning, containing rendered sensor modalities matching the observation of the agent, as well as the recorded action. The second is the “raw” unprocessed observations which include additional data from the VR recordings and simulator state of the activity-relevant objects.</p>
<p>Note: Since every small change in the simulator may cause deviating changes in execution, if you want to replay the demos (run the actions stored in the demonstration file, and reproduce the exact same outcomes, e.g. states/images), this comes with some caveats. You must:</p>
<ol class="simple">
<li><p>Be on Windows</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">pybullet-svl</span></code> fork (<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pybullet-svl</span></code>, as in setup.py of iGibson)</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">behavior-replay</span></code> branch of iGibson</p></li>
<li><p>Download the behavior-replay version of BEHAVIOR dataset bundle (BEHAVIOR Dataset of Objects and iGibson Dataset) via <a class="reference external" href="https://forms.gle/GXAacjpnotKkM2An7">this link</a></p></li>
</ol>
<p>Following the instructions above, we have verified that the 500 demos can be replayed and reproduced perfectly.</p>
<div class="section" id="behavior-hdf5-processed-dataset">
<h2>BEHAVIOR HDF5 Processed Dataset<a class="headerlink" href="#behavior-hdf5-processed-dataset" title="Permalink to this headline"></a></h2>
<p>We provide a processed dataset generated from the raw files and that includes images and proprioceptive sensing.
Alternative processed datasets can be generated based on the raw files, e.g. with different image resolutions.</p>
<div class="section" id="hdf5-content">
<h3>HDF5 Content<a class="headerlink" href="#hdf5-content" title="Permalink to this headline"></a></h3>
<p>The following are the available keys to index into the hdf5 file. The dimensionality of each component is noted parenthetically, where <code class="docutils literal notranslate"><span class="pre">N</span></code> indicates the number of frames in the demo.</p>
<ul class="simple">
<li><p>action (N x 28) – see BehaviorRobot description in the <a class="reference internal" href="agents.html"><span class="doc std std-doc">Embodiments section</span></a> for details about the actuation of this robot. This vector contains two additional dimensions that correspond to the <code class="docutils literal notranslate"><span class="pre">hand</span> <span class="pre">reset</span></code> action in VR: an action that teleports the simulated hands to the exact pose of the VR hand controller when they have diverged. This actions are not used by AI agents but are necessary to understand the demos.</p></li>
<li><p>proprioception (N x 20) – proprioceptive feedback. More details in the <a class="reference internal" href="agents.html"><span class="doc std std-doc">Embodiments section</span></a>.</p></li>
<li><p>rgb (N x 128 x 128 x 3) – rgb image from camera</p></li>
<li><p>depth (N x 128 x 128 x 1) – depth map</p></li>
<li><p>seg (N x 128 x 128 x 1) – segmentation of scene</p></li>
<li><p>ins_seg (N x 128 x 128 x 1) – instance segmentation</p></li>
<li><p>highlight ( N x 128 x 128 x 1) – activity relevant object binary mask, active for all objects included in the activity goal (except the agent and the floor)</p></li>
<li><p>task_obs (N x 456) – task observations, including ground truth state of the robot, and ground truth poses and grasping state of a maximum of a fixed number of activity relevant objects</p></li>
</ul>
</div>
</div>
<div class="section" id="behavior-hdf5-raw-dataset">
<h2>BEHAVIOR HDF5 Raw Dataset<a class="headerlink" href="#behavior-hdf5-raw-dataset" title="Permalink to this headline"></a></h2>
<div class="section" id="accessing-the-behavior-hdf5-demo-files">
<h3>Accessing the BEHAVIOR hdf5 demo files<a class="headerlink" href="#accessing-the-behavior-hdf5-demo-files" title="Permalink to this headline"></a></h3>
<p>This code snipped provides access to the data using the python’s hdf5 module that is dependency of iGibson:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>import h5py
import numpy

frame_number = 0
hf = h5py.File(‘/path/to/behavior/demo.hdf5’)

# see below for metadata description
activity_name = hf.attrs[‘metadata/task_name’]
print(activity_name)

# see below for finding indices for array
position_of_hmd = np.array(hf[‘frame_data’][‘vr_device_data’][‘hmd’][frame_number, 1:4])

</pre></div>
</div>
</div>
<div class="section" id="metadata">
<h3>Metadata<a class="headerlink" href="#metadata" title="Permalink to this headline"></a></h3>
<p>The metadata can be accessed by keying into the hdf5.attrs with the following keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/metadata/start_time</span></code>: the date the demo was recorded</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/metadata/physics_timestep</span></code>: the simulated time duration of each step of the physics simulator (1/300 seconds for all our demos)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/metadata/render_timestep</span></code>: the simulated time between each rendered image, determines the framerate (1/30 seconds). <code class="docutils literal notranslate"><span class="pre">render_timestep</span> <span class="pre">/</span> <span class="pre">physics_timestep</span></code> gives the number of physics simulation steps between two generated images (10)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/metadata/git_info</span></code>: the git info for activity definition, <code class="docutils literal notranslate"><span class="pre">iGibson</span></code>, <code class="docutils literal notranslate"><span class="pre">ig_dataset</span></code>, and <code class="docutils literal notranslate"><span class="pre">ig_assets</span></code>. This is used to ensure participants are using a compatible version of iGibson if replaying the demo</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/metadata/task_name</span></code>: The name of the activity, e.g. <code class="docutils literal notranslate"><span class="pre">washing_dishes</span></code>, <code class="docutils literal notranslate"><span class="pre">putting_away_groceries</span></code>…</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/metadata/task_instance</span></code>: The instance of the activity that specifies the state (pose, extended state) of the sampled activity relevant objects at initialization</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/metadata/scene_id</span></code>: The scene (<code class="docutils literal notranslate"><span class="pre">Rs_int</span></code>, <code class="docutils literal notranslate"><span class="pre">Wainscott_0_int</span></code>, etc.) where the activity was recorded</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/metadata/filter_objects</span></code>: Whether only activity relevant objects were recorded in the activity</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/metadata/obj_body_id_to_name</span></code>: mapping of pybullet IDs to the semantic object name</p></li>
</ul>
</div>
<div class="section" id="id1">
<h3>HDF5 Content<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>The following are the available keys to index into the hdf5 file. The dimensionality of each component is noted parenthetically, where <code class="docutils literal notranslate"><span class="pre">N</span></code> indicates the number of frames in the demo.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">frame_data</span></code> (N x 4)</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">goal_status</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">satisfied</span></code> (N x total_goals) – Total satisfied top-level predicates, where total_goals is the number of predicates</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">unsatisfied</span></code> (N x total_goals) – Total unsatisfied top-level predicates, where total_goals is the number of predicates</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">physics_data</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">string</span></code> (bullet_id: total number of activity-relevant scene objects)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">position</span></code> (N x 3) – The 3D position of the object center of mass</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">orientation</span></code> (N x 4) – The quaternion orientation of the object</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">joint_state</span></code> (N x number of object joints) – The pybullet joint state of each object</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">vr</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">vr_camera</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">right_eye_view</span></code> (N x 4 x 4) – the view projection matrix</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">right_eye_proj</span></code> (N x 4 x 4) – the camera projection matrix</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">right_camera_pos</span></code> (N x 3) – The 3D position of the camera</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">vr_device_data</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">hmd</span></code> (N x 17) – see below</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">left_controller</span></code> (N x 27) – see below</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">right_controller</span></code> (N x 27) – see below</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vr_position_data</span></code> (N x 12) – see below</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torso_tracker</span></code> (N x 8) – see below</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">vr_button_data</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">left_controller</span></code> (N x 3) – see below</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">right_controller</span></code> (N x 3) – see below</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">vr_eye_tracking_data</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">left_controller</span></code> (N x 9) – see below</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">vr_event_data</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">left_controller</span></code> (N x 28) – see below</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">right_controller</span></code> (N x 28) – see below</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reset_actions</span></code> (N x 2) – reset for left and right controller</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Agent_actions</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">vr_robot</span></code> (N x 28) – see BEHAVIOR robot description in previous section</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">action</span></code> – unused</p></li>
</ul>
<p>Additional description of the dimensions of the arrays noted above: the following are not keys but correspond to indices of the associated array:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">hmd</span></code> (17)</p>
<ul>
<li><p>hmd tracking data is valid (1)</p></li>
<li><p>translation (3)</p></li>
<li><p>rotation (4)</p></li>
<li><p>right vector (3)</p></li>
<li><p>up vector (3)</p></li>
<li><p>forward vector (3)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">left_controller</span></code>/<code class="docutils literal notranslate"><span class="pre">right_controller</span></code> (27)</p>
<ul>
<li><p>controller tracking data is valid (1)</p></li>
<li><p>translation (3)</p></li>
<li><p>rotation (4)</p></li>
<li><p>right vector (3)</p></li>
<li><p>up vector (3)</p></li>
<li><p>forward vector (3)</p></li>
<li><p>base_rotation (4)</p></li>
<li><p>base_rotation * controller_rotation (4)</p></li>
<li><p>applied_force (6) – p.getConstraintState(controller_constraint_id)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">vr_button_data</span></code></p>
<ul>
<li><p>trigger fraction (1) — open: 0 -&gt; closed: 1</p></li>
<li><p>touchpad x position (1) – left: -1 -&gt; right: 1</p></li>
<li><p>touchpad y position (1) – bottom: -1 -&gt; right: 1</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Vr_eye_tracking_data</span></code></p>
<ul>
<li><p>eye-tracking data is valid (1)</p></li>
<li><p>origin of gaze in world space (3)</p></li>
<li><p>direction vector of gaze in world space (3)</p></li>
<li><p>left pupil diameter (1)</p></li>
<li><p>right pupil diameter (1)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">vr_position_data</span></code> (12)</p>
<ul>
<li><p>position of the system in iGibson space (3)</p></li>
<li><p>offset of the system from the origin (3)</p></li>
<li><p>applied force to vr body (6) – p.getConstraintState(body_constraint_id)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">torso_tracker</span></code> (8)</p>
<ul>
<li><p>torso tracker is valid (1)</p></li>
<li><p>position (3)</p></li>
<li><p>rotation (4)</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="add-your-own-demos">
<h2>Add your own demos!<a class="headerlink" href="#add-your-own-demos" title="Permalink to this headline"></a></h2>
<p>Coming soon!</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="activities.html" class="btn btn-neutral float-left" title="The BEHAVIOR Dataset of Activity Definitions and BDDL" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="objects.html" class="btn btn-neutral float-right" title="The BEHAVIOR Dataset of Objects" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Stanford University 2018-2021.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>